{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask==2022.05.0 prefect==1.2.1\n"
     ]
    }
   ],
   "source": [
    "from prefect.utilities.logging import get_logger\n",
    "from prefect import task, Flow\n",
    "import requests\n",
    "import logging\n",
    "import prefect\n",
    "import dask\n",
    "from dask import distributed\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "print(f\"dask=={dask.__version__} prefect=={prefect.__version__}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaskHandler(prefect.utilities.logging.StreamHandler):\n",
    "    def __init__(self, prefix):\n",
    "        self._worker = None\n",
    "        self._prefix = prefix\n",
    "\n",
    "    def emit(self, record):\n",
    "        if self._worker is None:\n",
    "            self._worker = distributed.get_worker()\n",
    "        self._worker.log_event(self._prefix, {\"msg\": record.msg})\n",
    "        # requests.post(\"http://172.30.46.210:8000/\", params=dict(msg=record.msg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 17:20:32,666 - distributed.deploy.ssh - DEBUG - Created Scheduler Connection\n",
      "2022-05-24 17:20:34,025 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:34,023 - distributed.scheduler - INFO - State start\n",
      "2022-05-24 17:20:34,031 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:34,029 - distributed.scheduler - INFO - Clear task state\n",
      "2022-05-24 17:20:34,033 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:34,031 - distributed.scheduler - INFO -   Scheduler at: tcp://192.168.191.100:37149\n",
      "2022-05-24 17:20:34,035 - distributed.deploy.ssh - DEBUG - 2022-05-24 17:20:34,031 - distributed.scheduler - INFO -   Scheduler at: tcp://192.168.191.100:37149\n",
      "\n",
      "2022-05-24 17:20:34,038 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:34,039 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:34,048 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:34,049 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:35,032 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,031 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:35,033 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,031 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:35,036 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.191.100:44347'\n",
      "2022-05-24 17:20:35,042 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,041 - distributed.process - DEBUG - [<AsyncProcess Dask Worker process (from Nanny)>] got message {'op': 'start', 'future': <Future pending cb=[<TaskWakeupMethWrapper object at 0x7efd945b5640>()]>}\n",
      "2022-05-24 17:20:35,046 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,045 - distributed.process - DEBUG - [<AsyncProcess Dask Worker process (from Nanny)>] created process with pid 2334\n",
      "2022-05-24 17:20:35,595 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,594 - distributed.diskutils - DEBUG - Checking lock file '/home/salmon/dask-worker-space/worker-011m74ac.dirlock'...\n",
      "2022-05-24 17:20:35,596 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,594 - distributed.diskutils - INFO - Found stale lock file and directory '/home/salmon/dask-worker-space/worker-011m74ac', purging\n",
      "2022-05-24 17:20:35,598 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,594 - distributed.diskutils - DEBUG - Locking '/home/salmon/dask-worker-space/worker-qjob0s_z.dirlock'...\n",
      "2022-05-24 17:20:35,612 - distributed.deploy.ssh - INFO - 2022-05-24 17:20:35,610 - distributed.worker - INFO -       Start worker at: tcp://192.168.191.100:37613\n",
      "2022-05-24 17:20:35,613 - distributed.deploy.ssh - DEBUG - 2022-05-24 17:20:35,610 - distributed.worker - INFO -       Start worker at: tcp://192.168.191.100:37613\n",
      "\n",
      "2022-05-24 17:20:35,621 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:35,623 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:35,638 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:35,640 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "/fuyun/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages/distributed/client.py:1288: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+---------------+----------------+----------------+\n",
      "| Package | client        | scheduler      | workers        |\n",
      "+---------+---------------+----------------+----------------+\n",
      "| numpy   | 1.19.4        | 1.22.3         | 1.22.3         |\n",
      "| pandas  | 1.1.4         | None           | None           |\n",
      "| python  | 3.8.6.final.0 | 3.8.10.final.0 | 3.8.10.final.0 |\n",
      "+---------+---------------+----------------+----------------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n",
      "2022-05-24 17:20:35,654 - distributed.client - DEBUG - Started scheduling coroutines. Synchronized\n"
     ]
    }
   ],
   "source": [
    "cluster=distributed.SSHCluster([\"localhost\",\"localhost\"], connect_options={\"known_hosts\":None},silence_logs=False,worker_options={\"data\":{\"nodelist\":[\"a\",\"b\"]}})\n",
    "client=distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor=prefect.executors.DaskExecutor(address=cluster.scheduler.address,debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prefect.task(name=\"hello_world\", log_stdout=True)\n",
    "def hello_world():\n",
    "    print(\"Hello world!\")\n",
    "    # distributed.get_worker().log_event(\"runtime\", { \"msg\":\"I'm hello world!\"})\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "    logger.info(\"I'm hello world!\")\n",
    "\n",
    "    return \"balala\"\n",
    "\n",
    "\n",
    "@prefect.task(name=\"foo\", log_stdout=True)\n",
    "def foo(msg):\n",
    "    # worker = distributed.get_worker()\n",
    "    # data = worker.data[\"nodelist\"]\n",
    "    # worker.log_event(\"runtime\", {\"time\": time(), \"nodelist\": data})\n",
    "    logger=distributed.get_worker().data.get(\"logger\", None)\n",
    "    if logger is None:\n",
    "        logger = prefect.context.get(\"logger\")\n",
    "        logger.addHandler(DaskHandler(\"Foo\"))\n",
    "        distributed.get_worker().data[\"logger\"] = logger\n",
    "\n",
    "    logger.info(\"Just a test  {}\".format(msg))\n",
    "    # logger.removeHandler()\n",
    "    pprint(logging.Logger.manager.loggerDict.keys())\n",
    "    print(f\"Foo! {msg}\")\n",
    "\n",
    "\n",
    "with prefect.Flow(name=\"flow_test\") as flow:\n",
    "    foo(hello_world())\n",
    "\n",
    "# now attach our custom handler to Task B's logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 17:20:46+0800] INFO - prefect.FlowRunner | Beginning Flow run for 'flow_test'\n",
      "[2022-05-24 17:20:46+0800] INFO - prefect.DaskExecutor | Connecting to an existing Dask cluster at tcp://192.168.191.100:37149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 17:20:46,799 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:46,803 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:46,813 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:46,815 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:46,825 - distributed.client - DEBUG - Started scheduling coroutines. Synchronized\n",
      "2022-05-24 17:20:46,840 - distributed.client - DEBUG - Submit _maybe_run(...), hello_world-b1061b78e2e545afbd8b392bd5aab51c\n",
      "2022-05-24 17:20:46,856 - distributed.client - DEBUG - Submit _maybe_run(...), foo-f22382b7f17349a7ba0798895a1458df\n",
      "2022-05-24 17:20:46,863 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:46,864 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:46,866 - distributed.client - DEBUG - Waiting on futures to clear before gather\n",
      "2022-05-24 17:20:47,173 - distributed.client - DEBUG - Client receives message {'op': 'key-in-memory', 'key': 'hello_world-b1061b78e2e545afbd8b392bd5aab51c', 'type': b'\\x80\\x04\\x95$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x14prefect.engine.state\\x94\\x8c\\x07Success\\x94\\x93\\x94.'}\n",
      "2022-05-24 17:20:47,198 - distributed.client - DEBUG - Client receives message {'op': 'key-in-memory', 'key': 'foo-f22382b7f17349a7ba0798895a1458df', 'type': b'\\x80\\x04\\x95$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x14prefect.engine.state\\x94\\x8c\\x07Success\\x94\\x93\\x94.'}\n",
      "/fuyun/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages/prefect/executors/dask.py:329: RuntimeWarning: coroutine 'rpc.close_rpc' was never awaited\n",
      "  scheduler_comm.close_rpc()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2022-05-24 17:20:47,215 - distributed.client - DEBUG - Release key hello_world-b1061b78e2e545afbd8b392bd5aab51c\n",
      "2022-05-24 17:20:47,217 - distributed.client - DEBUG - Release key foo-f22382b7f17349a7ba0798895a1458df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 17:20:47+0800] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Success: \"All reference tasks succeeded.\">"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['concurrent.futures', 'concurrent', 'asyncio', 'tornado.access', 'tornado', 'tornado.application', 'tornado.general', 'prompt_toolkit.buffer', 'prompt_toolkit', 'parso.python.diff', 'parso.python', 'parso', 'parso.cache', 'IPKernelApp', 'prefect', 'prefect.NoResultType', 'urllib3.util.retry', 'urllib3.util', 'urllib3', 'urllib3.connection', 'urllib3.response', 'urllib3.connectionpool', 'urllib3.poolmanager', 'charset_normalizer', 'requests', 'prefect.FlowRunner', 'prefect.backend.flow_run', 'prefect.backend', 'prefect.backend.flow', 'distributed.config', 'distributed', 'distributed.client', 'bokeh', 'distributed.worker', 'distributed.utils', 'fsspec', 'distributed.protocol.compression', 'distributed.protocol', 'distributed.protocol.pickle', 'distributed.protocol.core', 'distributed.comm.core', 'distributed.comm', 'distributed.comm.utils', 'distributed.comm.inproc', 'distributed.comm.tcp', 'distributed.comm.ws', 'distributed.diagnostics.plugin', 'distributed.diagnostics', 'distributed.comm.ucx', 'distributed.core', 'distributed.preloading', 'distributed.batched', 'distributed.pubsub', 'distributed.sizeof', 'distributed.threadpoolexecutor', 'distributed.utils_comm', 'distributed.diskutils', 'distributed.utils_perf', 'distributed.spill', 'distributed.worker_memory', 'distributed.deploy.adaptive_core', 'distributed.deploy', 'distributed.deploy.adaptive', 'distributed.deploy.cluster', 'distributed.active_memory_manager', 'distributed.active_memory_manager.tasks', 'distributed.event', 'distributed.lock', 'distributed.multi_lock', 'distributed.queues', 'distributed.recreate_tasks', 'distributed.semaphore', 'distributed.stealing', 'distributed.variable', 'distributed.scheduler', 'distributed.deploy.spec', 'distributed.process', 'distributed.nanny', 'distributed.deploy.local', 'distributed.deploy.ssh', 'distributed.diagnostics.progress', 'distributed.diagnostics.progressbar', 'asyncssh', 'asyncssh.sftp', 'prefect.DaskExecutor', 'prefect.Task', 'prefect.hello_world', 'prefect.foo', 'prefect.flow_test', 'prefect.Result'])\n"
     ]
    }
   ],
   "source": [
    "pprint(logging.Logger.manager.loggerDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 17:20:58,336 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:58,337 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:58,342 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:20:58,343 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:20:58,356 - distributed.client - DEBUG - Started scheduling coroutines. Synchronized\n"
     ]
    }
   ],
   "source": [
    "client=distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 17:21:38+0800] INFO - prefect.FlowRunner | Beginning Flow run for 'flow_test'\n",
      "[2022-05-24 17:21:38+0800] INFO - prefect.DaskExecutor | Connecting to an existing Dask cluster at tcp://192.168.191.100:37149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 17:21:38,866 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:21:38,869 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:21:38,878 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:21:38,882 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:21:38,893 - distributed.client - DEBUG - Started scheduling coroutines. Synchronized\n",
      "2022-05-24 17:21:38,907 - distributed.client - DEBUG - Submit _maybe_run(...), hello_world-a7a47eeb67094ca4bb024959cddcbfac\n",
      "2022-05-24 17:21:38,922 - distributed.client - DEBUG - Submit _maybe_run(...), foo-491e044e837340fdad810a04f87ec1ca\n",
      "2022-05-24 17:21:38,928 - distributed.client - DEBUG - Waiting on futures to clear before gather\n",
      "2022-05-24 17:21:38,934 - distributed.comm.tcp - DEBUG - Setting TCP keepalive: nprobes=10, idle=10, interval=2\n",
      "2022-05-24 17:21:38,936 - distributed.comm.tcp - DEBUG - Setting TCP user timeout: 30000 ms\n",
      "2022-05-24 17:21:38,951 - distributed.client - DEBUG - Client receives message {'op': 'key-in-memory', 'key': 'hello_world-a7a47eeb67094ca4bb024959cddcbfac', 'type': b'\\x80\\x04\\x95$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x14prefect.engine.state\\x94\\x8c\\x07Success\\x94\\x93\\x94.'}\n",
      "2022-05-24 17:21:38,977 - distributed.client - DEBUG - Client receives message {'op': 'key-in-memory', 'key': 'foo-491e044e837340fdad810a04f87ec1ca', 'type': b'\\x80\\x04\\x95$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x14prefect.engine.state\\x94\\x8c\\x07Success\\x94\\x93\\x94.'}\n",
      "2022-05-24 17:21:38,990 - distributed.client - DEBUG - Release key hello_world-a7a47eeb67094ca4bb024959cddcbfac\n",
      "2022-05-24 17:21:38,992 - distributed.client - DEBUG - Release key foo-491e044e837340fdad810a04f87ec1ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-24 17:21:39+0800] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Success: \"All reference tasks succeeded.\">"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.run(executor= executor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1653384047.1904552, {'msg': 'Just a test  balala'}),\n",
      " (1653384073.0968456, {'msg': 'Just a test  balala'}),\n",
      " (1653384085.6051018, {'msg': 'Just a test  balala'}),\n",
      " (1653384092.1818671, {'msg': 'Just a test  balala'}),\n",
      " (1653384098.9724736, {'msg': 'Just a test  balala'}))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=TimeoutError('Timeout')>\n",
      "tornado.util.TimeoutError: Timeout\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=TimeoutError('Timeout')>\n",
      "tornado.util.TimeoutError: Timeout\n"
     ]
    }
   ],
   "source": [
    "pprint(client.get_events(\"prefect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with executor.start():\n",
    "    future=executor.submit(boo,\"what's up\")\n",
    "    res=executor.wait([future])\n",
    "    pprint(f\"result = {res}\")\n",
    "    pprint(executor.client.get_events(\"runtime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.client.get_events(\"runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect.engine.get_default_flow_runner_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.workers[0].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefect.engine.get_default_task_runner_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"Task A\")\n",
    "def task_a():\n",
    "    return 3\n",
    "\n",
    "\n",
    "@task(name=\"Task B\")\n",
    "def task_b(x):\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "    logger.info(\"Beginning to run Task B with input {}\".format(x))\n",
    "    y = 3 * x + 1\n",
    "    logger.info(\"Returning the value {}\".format(y))\n",
    "    return y\n",
    "\n",
    "\n",
    "with Flow(\"logging-example\") as flow:\n",
    "    result = task_b(task_a)\n",
    "\n",
    "\n",
    "# now attach our custom handler to Task B's logger\n",
    "task_logger = get_logger(\"Task B\")\n",
    "task_logger.addHandler(MyHandler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post(\"http://172.30.46.210:8000/\", params=dict(msg=\"Hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooTask(Task):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._cmd = ....\n",
    "        pass\n",
    "\n",
    "    def get_config(self):\n",
    "        pass\n",
    "\n",
    "    def run_raw(self):\n",
    "        self.get_config()\n",
    "        subprocess.exec(self._cmd)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.run_raw()\n",
    "        except RuntimeWarning as run_warning:\n",
    "            logger.warning(run_error)\n",
    "        except RuntimeError as run_error:\n",
    "            logger.error(run_error)\n",
    "        except Exception as error:\n",
    "            logger.err(error)\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ad1475237ae0817e601552d8b87bcccd09bed7a3d2283cd00b88633837ae647"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
